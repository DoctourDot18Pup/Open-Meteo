import os
import json
import pandas as pd
import glob
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataVerifier:
    def __init__(self):
        # Archivos CSV disponibles para verificar
        self.csv_files = {
            'historical_data': 'mexico_historical_weather_2020_2024_20251119.csv',
            'current_32_states': 'mexico_32_estados_daily_20251119_1717.csv',
            'recent_daily': 'mexico_reliable_daily_20251119.csv',
            'recent_hourly': 'mexico_reliable_hourly_20251119.csv'
        }
        
        # Directorios de procesamiento (si existen)
        self.processed_dirs = [
            'processed_weather_data_dynamo',
            'processed_monthly_metrics_dynamo', 
            'processed_alerts_dynamo'
        ]
    
    def verify_csv_files(self):
        """Verificar archivos CSV disponibles"""
        print("üìä VERIFICANDO ARCHIVOS CSV DISPONIBLES")
        print("="*60)
        
        for category, filename in self.csv_files.items():
            if os.path.exists(filename):
                try:
                    # Cargar CSV
                    df = pd.read_csv(filename)
                    file_size_kb = os.path.getsize(filename) / 1024
                    
                    print(f"‚úÖ {category.upper()}:")
                    print(f"   üìÅ Archivo: {filename}")
                    print(f"   üìè Tama√±o: {file_size_kb:.1f} KB")
                    print(f"   üìä Registros: {len(df):,}")
                    print(f"   üèõÔ∏è Columnas: {len(df.columns)}")
                    
                    # An√°lisis espec√≠fico por tipo de archivo
                    if 'state' in df.columns:
                        states = df['state'].nunique()
                        print(f"   üåé Estados √∫nicos: {states}")
                        
                    if 'date' in df.columns:
                        try:
                            df['date'] = pd.to_datetime(df['date'])
                            date_range = f"{df['date'].min().date()} a {df['date'].max().date()}"
                            print(f"   üìÖ Rango de fechas: {date_range}")
                        except:
                            print(f"   üìÖ Fechas: Formato no est√°ndar")
                    
                    if 'datetime' in df.columns:
                        try:
                            df['datetime'] = pd.to_datetime(df['datetime'])
                            date_range = f"{df['datetime'].min().date()} a {df['datetime'].max().date()}"
                            print(f"   üïê Rango temporal: {date_range}")
                        except:
                            print(f"   üïê Fechas: Formato no est√°ndar")
                    
                    # Verificar variables meteorol√≥gicas clave
                    key_vars = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum', 
                               'shortwave_radiation_sum', 'wind_speed_10m_max']
                    
                    available_vars = [var for var in key_vars if var in df.columns]
                    print(f"   üå°Ô∏è Variables meteorol√≥gicas: {len(available_vars)}/{len(key_vars)}")
                    
                    # Estad√≠sticas r√°pidas de temperatura
                    if 'temperature_2m_max' in df.columns:
                        temp_max_range = f"{df['temperature_2m_max'].min():.1f}¬∞C a {df['temperature_2m_max'].max():.1f}¬∞C"
                        print(f"   üî• Temp m√°xima: {temp_max_range}")
                    
                    if 'temperature_2m_min' in df.columns:
                        temp_min_range = f"{df['temperature_2m_min'].min():.1f}¬∞C a {df['temperature_2m_min'].max():.1f}¬∞C"
                        print(f"   üßä Temp m√≠nima: {temp_min_range}")
                    
                    if 'precipitation_sum' in df.columns:
                        precip_total = df['precipitation_sum'].sum()
                        print(f"   üåßÔ∏è Precipitaci√≥n total: {precip_total:.1f} mm")
                    
                    print()
                    
                except Exception as e:
                    print(f"‚ùå Error analizando {filename}: {e}")
                    print()
            else:
                print(f"‚ùå {category.upper()}: {filename} - NO ENCONTRADO")
                print()
    
    def verify_data_quality(self):
        """Verificar calidad de los datos principales"""
        print("üîç AN√ÅLISIS DE CALIDAD DE DATOS")
        print("="*60)
        
        # Verificar archivo principal de 32 estados
        main_file = self.csv_files['current_32_states']
        if os.path.exists(main_file):
            print(f"üìä AN√ÅLISIS DETALLADO: {main_file}")
            print("-" * 40)
            
            try:
                df = pd.read_csv(main_file)
                
                # An√°lisis por estado
                if 'state' in df.columns:
                    state_counts = df['state'].value_counts()
                    print(f"üìç DISTRIBUCI√ìN POR ESTADO:")
                    print(f"   Estados procesados: {len(state_counts)}/32")
                    print(f"   Registros por estado:")
                    
                    for state, count in state_counts.head(10).items():
                        print(f"      {state}: {count} registros")
                    
                    if len(state_counts) > 10:
                        print(f"      ... y {len(state_counts) - 10} estados m√°s")
                    print()
                
                # An√°lisis de completitud de variables
                print(f"üå°Ô∏è COMPLETITUD DE VARIABLES:")
                key_vars = ['temperature_2m_max', 'temperature_2m_min', 'precipitation_sum', 
                           'shortwave_radiation_sum', 'wind_speed_10m_max']
                
                for var in key_vars:
                    if var in df.columns:
                        non_null = df[var].notna().sum()
                        total = len(df)
                        completeness = (non_null / total) * 100
                        
                        if completeness > 95:
                            status = "‚úÖ"
                        elif completeness > 80:
                            status = "‚ö†Ô∏è"
                        else:
                            status = "‚ùå"
                        
                        print(f"   {status} {var}: {completeness:.1f}% ({non_null:,}/{total:,})")
                
                print()
                
                # An√°lisis de regiones
                if 'region' in df.columns:
                    region_counts = df['region'].value_counts()
                    print(f"üåé DISTRIBUCI√ìN POR REGI√ìN:")
                    for region, count in region_counts.items():
                        print(f"   {region}: {count} registros")
                    print()
                
                # Detecci√≥n de valores extremos
                if 'temperature_2m_max' in df.columns:
                    extreme_heat = df[df['temperature_2m_max'] > 45]
                    extreme_cold = df[df['temperature_2m_min'] < -10]
                    
                    print(f"üå°Ô∏è VALORES EXTREMOS:")
                    print(f"   D√≠as con calor extremo (>45¬∞C): {len(extreme_heat)}")
                    print(f"   D√≠as con fr√≠o extremo (<-10¬∞C): {len(extreme_cold)}")
                    
                    if len(extreme_heat) > 0:
                        hottest_day = extreme_heat.loc[extreme_heat['temperature_2m_max'].idxmax()]
                        print(f"   D√≠a m√°s caluroso: {hottest_day.get('state', 'Unknown')} - {hottest_day['temperature_2m_max']:.1f}¬∞C")
                    
                    if len(extreme_cold) > 0:
                        coldest_day = extreme_cold.loc[extreme_cold['temperature_2m_min'].idxmin()]
                        print(f"   D√≠a m√°s fr√≠o: {coldest_day.get('state', 'Unknown')} - {coldest_day['temperature_2m_min']:.1f}¬∞C")
                    
                    print()
                
            except Exception as e:
                print(f"‚ùå Error en an√°lisis de calidad: {e}")
        else:
            print(f"‚ùå Archivo principal no encontrado: {main_file}")
    
    def verify_agricultural_potential(self):
        """Verificar potencial para an√°lisis agr√≠cola"""
        print("üåæ AN√ÅLISIS DE POTENCIAL AGR√çCOLA")
        print("="*60)
        
        main_file = self.csv_files['current_32_states']
        if os.path.exists(main_file):
            try:
                df = pd.read_csv(main_file)
                
                # Variables necesarias para alertas agr√≠colas
                required_vars = {
                    'temperature_2m_max': 'Temperatura m√°xima',
                    'temperature_2m_min': 'Temperatura m√≠nima', 
                    'precipitation_sum': 'Precipitaci√≥n',
                    'shortwave_radiation_sum': 'Radiaci√≥n solar'
                }
                
                print("üîç VARIABLES REQUERIDAS PARA ALERTAS:")
                missing_vars = []
                
                for var, description in required_vars.items():
                    if var in df.columns:
                        non_null = df[var].notna().sum()
                        completeness = (non_null / len(df)) * 100
                        
                        if completeness > 90:
                            status = "‚úÖ"
                        elif completeness > 70:
                            status = "‚ö†Ô∏è"
                        else:
                            status = "‚ùå"
                            missing_vars.append(var)
                        
                        print(f"   {status} {description}: {completeness:.1f}% disponible")
                    else:
                        print(f"   ‚ùå {description}: NO DISPONIBLE")
                        missing_vars.append(var)
                
                # Evaluaci√≥n de viabilidad
                print(f"\nüìä EVALUACI√ìN DE VIABILIDAD:")
                if len(missing_vars) == 0:
                    print("   ‚úÖ EXCELENTE - Todos los datos necesarios disponibles")
                    print("   üöÄ Listo para procesamiento PySpark y generaci√≥n de alertas")
                elif len(missing_vars) <= 1:
                    print("   ‚ö†Ô∏è BUENO - La mayor√≠a de datos disponibles")
                    print("   üîß Se pueden generar alertas con limitaciones menores")
                else:
                    print("   ‚ùå LIMITADO - Faltan variables cr√≠ticas")
                    print("   üìù Recomendado: Obtener m√°s datos antes del procesamiento")
                
                # An√°lisis de cobertura temporal
                if 'date' in df.columns:
                    try:
                        df['date'] = pd.to_datetime(df['date'])
                        days_span = (df['date'].max() - df['date'].min()).days
                        unique_dates = df['date'].nunique()
                        
                        print(f"\nüìÖ COBERTURA TEMPORAL:")
                        print(f"   Per√≠odo total: {days_span} d√≠as")
                        print(f"   Fechas √∫nicas: {unique_dates}")
                        print(f"   Cobertura: {(unique_dates/days_span)*100:.1f}%")
                        
                        if unique_dates >= 30:
                            print("   ‚úÖ Suficiente para an√°lisis de tendencias")
                        else:
                            print("   ‚ö†Ô∏è Per√≠odo corto para an√°lisis robusto")
                            
                    except:
                        print(f"\nüìÖ COBERTURA TEMPORAL: No se pudo analizar formato de fecha")
                
                print()
                
            except Exception as e:
                print(f"‚ùå Error en an√°lisis agr√≠cola: {e}")
        else:
            print(f"‚ùå Archivo principal no disponible para an√°lisis")
    
    def verify_directory_structure(self):
        """Verificar que los directorios procesados existen"""
        print("üîç VERIFICANDO ESTRUCTURA DE DATOS PROCESADOS")
        print("="*60)
        
        for dir_name in self.processed_dirs:
            if os.path.exists(dir_name):
                files = os.listdir(dir_name)
                json_files = [f for f in files if f.endswith('.json')]
                
                print(f"‚úÖ {dir_name}/")
                print(f"   üìÅ Archivos totales: {len(files)}")
                print(f"   üìÑ Archivos JSON: {len(json_files)}")
                
                # Mostrar nombres de archivos
                for file in json_files[:3]:  # Solo los primeros 3
                    file_path = os.path.join(dir_name, file)
                    size_kb = os.path.getsize(file_path) / 1024
                    print(f"   üìã {file} ({size_kb:.1f} KB)")
                
                if len(json_files) > 3:
                    print(f"   ... y {len(json_files) - 3} archivos m√°s")
                    
            else:
                print(f"‚ùå {dir_name}/ - NO ENCONTRADO")
            print()
    
    def load_json_file(self, file_path):
        """Cargar archivo JSON con manejo de errores"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return data
        except Exception as e:
            logger.error(f"Error cargando {file_path}: {e}")
            return None
    
    def verify_weather_data(self):
        """Verificar datos meteorol√≥gicos procesados"""
        print("üå§Ô∏è VERIFICANDO DATOS METEOROL√ìGICOS PROCESADOS")
        print("="*50)
        
        weather_dir = 'processed_weather_data_dynamo'
        if not os.path.exists(weather_dir):
            print("‚ùå Directorio de datos meteorol√≥gicos no encontrado")
            return
            
        json_files = glob.glob(os.path.join(weather_dir, '*.json'))
        
        if not json_files:
            print("‚ùå No se encontraron archivos JSON")
            return
            
        # Leer primer archivo JSON
        sample_file = json_files[0]
        print(f"üìã Analizando: {os.path.basename(sample_file)}")
        
        try:
            # Leer l√≠nea por l√≠nea (formato JSON Lines)
            records = []
            with open(sample_file, 'r') as f:
                for line in f:
                    if line.strip():
                        records.append(json.loads(line))
            
            print(f"üìä Total de registros: {len(records)}")
            
            if records:
                sample_record = records[0]
                print("\nüìã ESTRUCTURA DE REGISTRO:")
                for key, value in sample_record.items():
                    print(f"   {key}: {value} ({type(value).__name__})")
                
        except Exception as e:
            print(f"‚ùå Error analizando archivo: {e}")
    
    def verify_alerts_data(self):
        """Verificar datos de alertas"""
        print("\n‚ö†Ô∏è VERIFICANDO DATOS DE ALERTAS")
        print("="*50)
        
        alerts_dir = 'processed_alerts_dynamo'
        if not os.path.exists(alerts_dir):
            print("‚ùå Directorio de alertas no encontrado")
            return
            
        json_files = glob.glob(os.path.join(alerts_dir, '*.json'))
        
        if not json_files:
            print("‚ùå No se encontraron archivos de alertas")
            return
            
        sample_file = json_files[0]
        print(f"üìã Analizando: {os.path.basename(sample_file)}")
        
        try:
            # Leer alertas
            alerts = []
            with open(sample_file, 'r') as f:
                for line in f:
                    if line.strip():
                        alerts.append(json.loads(line))
            
            print(f"‚ö†Ô∏è Total de alertas: {len(alerts)}")
            
        except Exception as e:
            print(f"‚ùå Error analizando alertas: {e}")
    
    def verify_metrics_data(self):
        """Verificar m√©tricas agregadas"""
        print("\nüìà VERIFICANDO M√âTRICAS AGREGADAS")
        print("="*50)
        
        metrics_dir = 'processed_monthly_metrics_dynamo'
        if not os.path.exists(metrics_dir):
            print("‚ùå Directorio de m√©tricas no encontrado")
            return
            
        json_files = glob.glob(os.path.join(metrics_dir, '*.json'))
        
        if not json_files:
            print("‚ùå No se encontraron archivos de m√©tricas")
            return
            
        sample_file = json_files[0]
        print(f"üìã Analizando: {os.path.basename(sample_file)}")
        print("‚úÖ M√©tricas encontradas")
    
    def validate_data_integrity(self):
        """Validar integridad general de los datos"""
        print("\n‚úÖ VALIDACI√ìN DE INTEGRIDAD")
        print("="*50)
        
        issues = []
        
        # Verificar que todos los directorios existen
        for dir_name in self.processed_dirs:
            if not os.path.exists(dir_name):
                issues.append(f"Falta directorio: {dir_name}")
            else:
                json_files = glob.glob(os.path.join(dir_name, '*.json'))
                if not json_files:
                    issues.append(f"No hay archivos JSON en: {dir_name}")
        
        if issues:
            print("‚ö†Ô∏è PROBLEMAS DETECTADOS:")
            for issue in issues:
                print(f"   ‚ùå {issue}")
        else:
            print("‚úÖ Todos los archivos parecen estar en buen estado")
            
        return len(issues) == 0
    
    def generate_summary_report(self):
        """Generar reporte resumen completo"""
        print("\n" + "="*70)
        print("üìä REPORTE FINAL DE VERIFICACI√ìN")
        print("="*70)
        
        total_size = 0
        total_files = 0
        
        for dir_name in self.processed_dirs:
            if os.path.exists(dir_name):
                json_files = glob.glob(os.path.join(dir_name, '*.json'))
                dir_size = sum(os.path.getsize(f) for f in json_files)
                total_size += dir_size
                total_files += len(json_files)
                
                print(f"üìÅ {dir_name}:")
                print(f"   Archivos: {len(json_files)}")
                print(f"   Tama√±o: {dir_size / 1024:.1f} KB")
        
        print(f"\nüìä TOTALES:")
        print(f"   üìÑ Archivos JSON generados: {total_files}")
        print(f"   üíæ Espacio total: {total_size / 1024:.1f} KB")
        print(f"   ‚úÖ Estado: Listos para DynamoDB")

def main():
    """Funci√≥n principal de verificaci√≥n"""
    print("üîç VERIFICADOR DE DATOS CSV - OPENMETEO PROJECT")
    print("="*70)
    print(f"üïê Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    verifier = DataVerifier()
    
    # 1. Verificar archivos CSV disponibles
    verifier.verify_csv_files()
    
    # 2. An√°lisis de calidad de datos
    verifier.verify_data_quality()
    
    # 3. Verificar potencial agr√≠cola
    verifier.verify_agricultural_potential()
    
    # 4. Verificar si existen datos procesados (opcional)
    if any(os.path.exists(dir_name) for dir_name in verifier.processed_dirs):
        print("üìÅ VERIFICANDO DATOS PROCESADOS EXISTENTES")
        print("="*60)
        verifier.verify_directory_structure()
        if os.path.exists('processed_weather_data_dynamo'):
            verifier.verify_weather_data()
        if os.path.exists('processed_alerts_dynamo'):
            verifier.verify_alerts_data()
        if os.path.exists('processed_monthly_metrics_dynamo'):
            verifier.verify_metrics_data()
        
        # Validaci√≥n de integridad procesados
        verifier.validate_data_integrity()
        verifier.generate_summary_report()
    else:
        print("\nüìã ESTADO ACTUAL: DATOS CSV LISTOS")
        print("="*50)
        print("‚úÖ Archivos CSV disponibles y verificados")
        print("üöÄ PR√ìXIMO PASO: Ejecutar pipeline PySpark")
        print("üì¶ Comando sugerido: python pyspark_agricultural_processor.py")
        print()
        
        # Resumen de archivos principales
        available_files = []
        for category, filename in verifier.csv_files.items():
            if os.path.exists(filename):
                available_files.append(f"‚úÖ {category}: {filename}")
            else:
                available_files.append(f"‚ùå {category}: {filename}")
        
        print("üìä ARCHIVOS DISPONIBLES:")
        for file_status in available_files:
            print(f"   {file_status}")
        
        print("\nüéØ RECOMENDACI√ìN:")
        if os.path.exists('mexico_32_estados_daily_20251119_1717.csv'):
            print("   ‚úÖ Archivo principal de 32 estados disponible")
            print("   üöÄ Proceder con procesamiento PySpark inmediatamente")
        else:
            print("   ‚ö†Ô∏è Ejecutar primero: python reliable_extractor_32_states.py")
            print("   üìä Para generar datos de los 32 estados")

if __name__ == "__main__":
    main()
